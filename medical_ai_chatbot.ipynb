{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**# Step 1: Load Dataset**"
      ],
      "metadata": {
        "id": "pcostf0vMlT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVb0SSsOLZQV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# Load dataset\n",
        "file_path = \"/content/sample_data/ai-medical-chatbot.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **# Step 2: Text** **Preprocessing**"
      ],
      "metadata": {
        "id": "VpkxbwpYMvmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9 ]', '', text.lower())\n",
        "\n",
        "# Apply text cleaning\n",
        "df['Patient'] = df['Patient'].astype(str).apply(clean_text)\n",
        "df['Doctor'] = df['Doctor'].astype(str).apply(clean_text)\n",
        "\n",
        "# Display cleaned text\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wDoT70SIL4bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **# Step 3: Tokenization and Padding**"
      ],
      "metadata": {
        "id": "6Y0edLBONJx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenizing the patient (input) text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Patient'])\n",
        "X = pad_sequences(tokenizer.texts_to_sequences(df['Patient']), padding='post')\n",
        "\n",
        "# Tokenizing the doctor (output) text\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(df['Doctor'])\n",
        "y_sequences = y_tokenizer.texts_to_sequences(df['Doctor'])\n",
        "\n",
        "# Convert output sequences to categorical labels\n",
        "y = np.array([seq[0] if seq else 0 for seq in y_sequences])\n",
        "\n",
        "# Display tokenized output\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "hbPsoW4FMGl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **# Step 4: Train-Test Split**"
      ],
      "metadata": {
        "id": "s1SYMYC2NTpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display dataset sizes\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
      ],
      "metadata": {
        "id": "qRZbXFUwMNtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **# Step 5: Define and Train the Model**"
      ],
      "metadata": {
        "id": "vpmGN4nXNgQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "# Function to create the model\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=X.shape[1]),\n",
        "        LSTM(128),\n",
        "        Dense(len(y_tokenizer.word_index) + 1, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=16)\n",
        "\n",
        "# Save model\n",
        "model.save('/mnt/data/health_chatbot.h5')"
      ],
      "metadata": {
        "id": "2CQk0lr1MWsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**#Step 6: Deploy Chatbot using Flask**"
      ],
      "metadata": {
        "id": "_we8roycNu6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_input = request.json['question']\n",
        "    seq = pad_sequences(tokenizer.texts_to_sequences([clean_text(user_input)]), maxlen=X.shape[1], padding='post')\n",
        "    prediction = model.predict(seq)\n",
        "    response_index = np.argmax(prediction)\n",
        "    response = y_tokenizer.index_word.get(response_index, 'Sorry, I am not sure.')\n",
        "    return jsonify({'response': response})\n",
        "\n",
        "# Run the app\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "Ftk-l22ZMeK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}